#!/bin/bash

# PH2 Dataset LoRA Training Script - Attention + Embedding Strategy
# ç­–ç•¥ï¼šåŒæ™‚è¨“ç·´ Attention (QKV + Projection) å’Œ Embedding å±¤

# 1. åŸºæœ¬è·¯å¾‘èˆ‡ç‰ˆæœ¬è¨­å®š
VERSION="old"
DATA_NAME="PH2"
DATA_DIR="./data/PH2/Train"
OUT_DIR="./results/ph2_lora_attn_emb_r32_T50_DPM"
RESUME_CHECKPOINT="./emasavedmodel_step1000.pt"

# 2. LoRA æ ¸å¿ƒåƒæ•¸
USE_LORA="True"
LORA_RANK=32
LORA_ALPHA=64.0
LORA_DROPOUT=0.1
LORA_TARGET="attn_emb"  # æ–°å¢ï¼šæ³¨å…¥ Attention + Embedding å±¤

# 3. è¨“ç·´è¶…åƒæ•¸
BATCH_SIZE=3
LR=2e-4
SAVE_INTERVAL=500
IMAGE_SIZE=256

# 4. æ“´æ•£æ¨¡å‹è¨­å®š
DIFFUSION_STEPS=50
DPM_SOLVER="True"

# å‰µå»ºè¼¸å‡ºç›®éŒ„
mkdir -p $OUT_DIR

echo "ğŸš€ Starting LoRA Training with Attention + Embedding Strategy"
echo "   Target: $LORA_TARGET (QKV + Projection + Embedding)"
echo "   Rank: $LORA_RANK, Alpha: $LORA_ALPHA"
echo "   Output: $OUT_DIR"

# åŸ·è¡Œè¨“ç·´
python scripts/segmentation_train.py \
    --version $VERSION \
    --data_name $DATA_NAME \
    --data_dir $DATA_DIR \
    --out_dir $OUT_DIR \
    --image_size $IMAGE_SIZE \
    --num_channels 128 \
    --class_cond False \
    --num_res_blocks 2 \
    --num_heads 1 \
    --learn_sigma True \
    --use_scale_shift_norm False \
    --attention_resolutions 16 \
    --diffusion_steps $DIFFUSION_STEPS \
    --dpm_solver $DPM_SOLVER \
    --noise_schedule linear \
    --rescale_learned_sigmas False \
    --rescale_timesteps False \
    --resume_checkpoint $RESUME_CHECKPOINT \
    --use_lora $USE_LORA \
    --lora_rank $LORA_RANK \
    --lora_alpha $LORA_ALPHA \
    --lora_dropout $LORA_DROPOUT \
    --lora_target_modules $LORA_TARGET \
    --batch_size $BATCH_SIZE \
    --lr $LR \
    --save_interval $SAVE_INTERVAL \
    --gpu_dev "0"

echo "âœ… LoRA Training (Attention + Embedding) completed! Results saved to $OUT_DIR"
