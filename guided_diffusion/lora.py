"""
LoRA (Low-Rank Adaptation) å¯¦ä½œ

æ­¤æ¨¡çµ„æä¾› LoRA å±¤çš„å¯¦ä½œå’Œè‡ªå‹•æ³¨å…¥æ©Ÿåˆ¶ï¼Œç”¨æ–¼é«˜æ•ˆå¾®èª¿é è¨“ç·´æ¨¡å‹ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. LoRALayer: ä½ç§©é©æ‡‰å±¤
2. LinearWithLoRA: åŒ…è£ Linear å±¤çš„ LoRA ç‰ˆæœ¬
3. inject_lora: è‡ªå‹•å°‡ LoRA æ³¨å…¥æ¨¡å‹
4. count_lora_parameters: çµ±è¨ˆ LoRA åƒæ•¸é‡
"""

import math
import torch
import torch.nn as nn
import torch.nn.functional as F


class LoRALayer(nn.Module):
    """
    LoRA ä½ç§©é©æ‡‰å±¤
    
    å¯¦ç¾å…¬å¼: h = W_0 * x + (alpha / r) * B * A * x
    å…¶ä¸­:
        - W_0 æ˜¯å‡çµçš„é è¨“ç·´æ¬Šé‡
        - A âˆˆ R^{r Ã— in_features}, B âˆˆ R^{out_features Ã— r}
        - r æ˜¯ç§© (rank)
        - alpha æ˜¯ç¸®æ”¾å› å­
    
    Args:
        in_features: è¼¸å…¥ç‰¹å¾µç¶­åº¦
        out_features: è¼¸å‡ºç‰¹å¾µç¶­åº¦
        rank: ä½ç§©åˆ†è§£çš„ç§©
        alpha: ç¸®æ”¾è¶…åƒæ•¸
        dropout: Dropout æ¯”ç‡
    """
    
    def __init__(self, in_features, out_features, rank=4, alpha=1.0, dropout=0.0):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.rank = rank
        self.alpha = alpha
        
        # LoRA æ¬Šé‡: A å’Œ B
        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))
        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))
        
        # Dropout (å¯é¸)
        self.dropout = nn.Dropout(p=dropout) if dropout > 0 else nn.Identity()
        
        # ç¸®æ”¾å› å­
        self.scaling = alpha / rank
        
        # åˆå§‹åŒ–
        self.reset_parameters()
    
    def reset_parameters(self):
        """åˆå§‹åŒ– LoRA åƒæ•¸"""
        # A: Kaiming uniform åˆå§‹åŒ–
        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))
        # B: é›¶åˆå§‹åŒ– (ç¢ºä¿åˆå§‹æ™‚ LoRA ä¸å½±éŸ¿åŸæ¨¡å‹)
        nn.init.zeros_(self.lora_B)
    
    def forward(self, x):
        """
        å‰å‘å‚³æ’­
        
        Args:
            x: è¼¸å…¥å¼µé‡, shape = (batch_size, ..., in_features)
        
        Returns:
            LoRA è¼¸å‡º, shape = (batch_size, ..., out_features)
        """
        # h = (alpha / r) * B * A * x
        x = self.dropout(x)
        # (batch, ..., in_features) @ (in_features, rank) -> (batch, ..., rank)
        h = F.linear(x, self.lora_A)
        # (batch, ..., rank) @ (rank, out_features) -> (batch, ..., out_features)
        h = F.linear(h, self.lora_B)
        # ç¸®æ”¾
        h = h * self.scaling
        return h


class LinearWithLoRA(nn.Module):
    """
    å°‡ LoRA å±¤åŒ…è£åˆ°åŸå§‹ Linear å±¤çš„åŒ…è£å™¨
    
    å‰å‘å‚³æ’­: output = W_0 * x + LoRA(x)
    
    Args:
        linear: åŸå§‹çš„ nn.Linear å±¤ (å°‡è¢«å‡çµ)
        rank: LoRA çš„ç§©
        alpha: LoRA çš„ç¸®æ”¾å› å­
        dropout: LoRA çš„ dropout æ¯”ç‡
    """
    
    def __init__(self, linear, rank=4, alpha=1.0, dropout=0.0):
        super().__init__()
        self.linear = linear
        self.lora = LoRALayer(
            in_features=linear.in_features,
            out_features=linear.out_features,
            rank=rank,
            alpha=alpha,
            dropout=dropout
        )
        
        # å‡çµåŸå§‹æ¬Šé‡
        self.linear.weight.requires_grad = False
        if self.linear.bias is not None:
            self.linear.bias.requires_grad = False
    
    def forward(self, x):
        """å‰å‘å‚³æ’­: åŸå§‹å±¤ + LoRA"""
        return self.linear(x) + self.lora(x)


class Conv1dWithLoRA(nn.Module):
    """
    å°‡ LoRA å±¤åŒ…è£åˆ°åŸå§‹ Conv1d å±¤çš„åŒ…è£å™¨
    
    å°ˆé–€ç”¨æ–¼ UNet Attention ä¸­çš„ QKV å’ŒæŠ•å½±å±¤
    å‰å‘å‚³æ’­: output = W_0 * x + LoRA(x)
    
    Args:
        conv1d: åŸå§‹çš„ nn.Conv1d å±¤ (å°‡è¢«å‡çµ)
        rank: LoRA çš„ç§©
        alpha: LoRA çš„ç¸®æ”¾å› å­
        dropout: LoRA çš„ dropout æ¯”ç‡
    """
    
    def __init__(self, conv1d, rank=4, alpha=1.0, dropout=0.0):
        super().__init__()
        self.conv1d = conv1d
        
        # å°æ–¼ 1x1 å·ç©ï¼Œå¯ä»¥è¦–ç‚º Linear å±¤
        # Conv1d: (out_channels, in_channels, kernel_size)
        # ç­‰åƒ¹æ–¼ Linear: (out_channels, in_channels) when kernel_size=1
        assert conv1d.kernel_size == (1,), "LoRA only supports 1x1 Conv1d"
        
        self.lora = LoRALayer(
            in_features=conv1d.in_channels,
            out_features=conv1d.out_channels,
            rank=rank,
            alpha=alpha,
            dropout=dropout
        )
        
        # å‡çµåŸå§‹æ¬Šé‡
        self.conv1d.weight.requires_grad = False
        if self.conv1d.bias is not None:
            self.conv1d.bias.requires_grad = False
    
    def forward(self, x):
        """
        å‰å‘å‚³æ’­: åŸå§‹ Conv1d + LoRA
        
        Args:
            x: (batch, in_channels, seq_len)
        Returns:
            (batch, out_channels, seq_len)
        """
        # åŸå§‹å·ç©è¼¸å‡º
        h = self.conv1d(x)
        
        # LoRA è¼¸å‡º (éœ€è¦è½‰æ›ç¶­åº¦)
        # x: (batch, in_channels, seq_len) -> (batch, seq_len, in_channels)
        x_permuted = x.permute(0, 2, 1)
        # LoRA è™•ç†: (batch, seq_len, in_channels) -> (batch, seq_len, out_channels)
        lora_out = self.lora(x_permuted)
        # è½‰å›: (batch, seq_len, out_channels) -> (batch, out_channels, seq_len)
        lora_out = lora_out.permute(0, 2, 1)
        
        return h + lora_out


def inject_lora(model, rank=4, alpha=1.0, dropout=0.0, target_modules='emb_only'):
    """
    å°‡ LoRA å±¤è‡ªå‹•æ³¨å…¥åˆ°æ¨¡å‹ä¸­
    
    Args:
        model: è¦æ³¨å…¥ LoRA çš„æ¨¡å‹
        rank: LoRA çš„ç§©
        alpha: LoRA çš„ç¸®æ”¾å› å­
        dropout: LoRA çš„ dropout æ¯”ç‡
        target_modules: è¦æ³¨å…¥ LoRA çš„æ¨¡çµ„ç­–ç•¥ï¼Œå¯é¸:
                       - 'emb_only': åªæ³¨å…¥ Embedding å±¤ (é è¨­ï¼Œæœ€ä¿å®ˆ)
                       - 'attn_only': åªæ³¨å…¥ Attention çš„ QKV å’ŒæŠ•å½±å±¤
                       - 'attn_emb': æ³¨å…¥ Attention + Embedding (æ¨è–¦)
                       - æˆ–è€…è‡ªå®šç¾©åˆ—è¡¨: ['qkv', 'proj_out', 'emb_layers']
    
    Returns:
        æ³¨å…¥ LoRA å¾Œçš„æ¨¡å‹
    """
    # ç­–ç•¥æ˜ å°„
    strategy_map = {
        'emb_only': ['emb_layers'],
        'attn_only': ['qkv', 'proj_out'],
        'attn_emb': ['qkv', 'proj_out', 'emb_layers'],
    }
    
    # è§£æ target_modules
    if isinstance(target_modules, str):
        if target_modules in strategy_map:
            target_modules = strategy_map[target_modules]
            strategy_name = target_modules
        else:
            raise ValueError(f"Unknown strategy '{target_modules}'. Choose from: {list(strategy_map.keys())}")
    elif isinstance(target_modules, list):
        strategy_name = 'custom'
    else:
        target_modules = strategy_map['emb_only']
        strategy_name = 'emb_only'
    
    print(f"ğŸ” Injecting LoRA (rank={rank}, alpha={alpha}, dropout={dropout}) into model...")
    print(f"   Strategy: {strategy_name}")
    print(f"   Target modules: {target_modules}")
    
    # ç¬¬ä¸€éšæ®µ: æ”¶é›†æ‰€æœ‰éœ€è¦ä¿®æ”¹çš„æ¨¡çµ„
    modules_to_modify = []
    
    for name, module in model.named_modules():
        # æª¢æŸ¥æ˜¯å¦ç‚ºç›®æ¨™æ¨¡çµ„
        should_inject = any(target in name for target in target_modules)
        
        if not should_inject:
            continue
        
        # æ‰¾åˆ°çˆ¶æ¨¡çµ„å’Œå­æ¨¡çµ„åç¨±
        parent_name = '.'.join(name.split('.')[:-1])
        child_name = name.split('.')[-1]
        
        if parent_name:
            parent_module = dict(model.named_modules())[parent_name]
        else:
            parent_module = model
        
        # æ”¯æŒ Linear å’Œ Conv1d å±¤
        if isinstance(module, nn.Linear):
            modules_to_modify.append(('linear', parent_module, child_name, module, name))
        elif isinstance(module, nn.Conv1d) and module.kernel_size == (1,):
            modules_to_modify.append(('conv1d', parent_module, child_name, module, name))
    
    # ç¬¬äºŒéšæ®µ: çµ±ä¸€æ›¿æ›
    injected_count = 0
    linear_count = 0
    conv1d_count = 0
    
    for module_type, parent_module, child_name, child, full_name in modules_to_modify:
        if module_type == 'linear':
            lora_layer = LinearWithLoRA(child, rank=rank, alpha=alpha, dropout=dropout)
            linear_count += 1
        elif module_type == 'conv1d':
            lora_layer = Conv1dWithLoRA(child, rank=rank, alpha=alpha, dropout=dropout)
            conv1d_count += 1
        
        setattr(parent_module, child_name, lora_layer)
        injected_count += 1
        print(f"   âœ“ Injected LoRA into: {full_name}")
    
    print(f"âœ… Successfully injected LoRA into {injected_count} layers!")
    print(f"   - Linear layers: {linear_count}")
    print(f"   - Conv1d layers: {conv1d_count}")
    
    # å‡çµæ‰€æœ‰é LoRA åƒæ•¸
    freeze_non_lora_parameters(model)
    
    return model


def freeze_non_lora_parameters(model):
    """
    å‡çµæ‰€æœ‰é LoRA çš„åƒæ•¸
    
    Args:
        model: æ¨¡å‹
    """
    frozen_count = 0
    lora_count = 0
    
    for name, param in model.named_parameters():
        if 'lora' in name:
            param.requires_grad = True
            lora_count += 1
        else:
            param.requires_grad = False
            frozen_count += 1
    
    print(f"â„ï¸  Frozen {frozen_count} non-LoRA parameters")
    print(f"ğŸ”¥ Kept {lora_count} LoRA parameters trainable")


def count_lora_parameters(model):
    """
    çµ±è¨ˆæ¨¡å‹ä¸­çš„åƒæ•¸é‡
    
    Args:
        model: æ¨¡å‹
    
    Returns:
        dict: åŒ…å«ä»¥ä¸‹éµçš„å­—å…¸
            - total_params: ç¸½åƒæ•¸é‡
            - trainable_params: å¯è¨“ç·´åƒæ•¸é‡
            - lora_params: LoRA åƒæ•¸é‡
            - trainable_percentage: å¯è¨“ç·´åƒæ•¸ç™¾åˆ†æ¯”
    """
    total_params = 0
    trainable_params = 0
    lora_params = 0
    
    for name, param in model.named_parameters():
        num_params = param.numel()
        total_params += num_params
        
        if param.requires_grad:
            trainable_params += num_params
            
        if 'lora' in name:
            lora_params += num_params
    
    trainable_percentage = (trainable_params / total_params) * 100 if total_params > 0 else 0
    
    return {
        'total_params': total_params,
        'trainable_params': trainable_params,
        'lora_params': lora_params,
        'trainable_percentage': trainable_percentage
    }


def print_lora_parameters(model):
    """
    æ‰“å° LoRA åƒæ•¸çµ±è¨ˆ
    
    Args:
        model: æ¨¡å‹
    """
    stats = count_lora_parameters(model)
    
    print("\n" + "="*60)
    print("ğŸ“Š Parameter Statistics")
    print("="*60)
    print(f"Total parameters:      {stats['total_params']:,}")
    print(f"Trainable parameters:  {stats['trainable_params']:,}")
    print(f"LoRA parameters:       {stats['lora_params']:,}")
    print(f"Trainable percentage:  {stats['trainable_percentage']:.2f}%")
    print("="*60 + "\n")


def merge_lora_weights(model):
    """
    å°‡ LoRA æ¬Šé‡åˆä½µåˆ°åŸå§‹æ¬Šé‡ä¸­
    
    é€™æœƒå°‡ LoRA çš„ä½ç§©æ›´æ–°åˆä½µåˆ°åŸå§‹çš„ Linear å±¤ï¼Œ
    ä¹‹å¾Œå¯ä»¥ç§»é™¤ LoRA å±¤ä»¥ç¯€çœæ¨ç†æ™‚çš„è¨ˆç®—æˆæœ¬ã€‚
    
    Args:
        model: åŒ…å« LoRA å±¤çš„æ¨¡å‹
    
    Returns:
        åˆä½µå¾Œçš„æ¨¡å‹
    """
    print("ğŸ”€ Merging LoRA weights into base model...")
    
    merged_count = 0
    for name, module in model.named_modules():
        if isinstance(module, LinearWithLoRA):
            # è¨ˆç®—åˆä½µå¾Œçš„æ¬Šé‡: W' = W_0 + (alpha/r) * B @ A
            with torch.no_grad():
                lora_weight = module.lora.lora_B @ module.lora.lora_A
                lora_weight = lora_weight * module.lora.scaling
                module.linear.weight.data += lora_weight
                
                # è§£å‡åƒæ•¸
                module.linear.weight.requires_grad = True
                if module.linear.bias is not None:
                    module.linear.bias.requires_grad = True
            
            merged_count += 1
    
    print(f"âœ… Merged {merged_count} LoRA layers!")
    return model


def detect_lora_in_state_dict(state_dict):
    """
    å¾ state_dict ä¸­æª¢æ¸¬æ˜¯å¦åŒ…å« LoRA æ¬Šé‡
    
    Args:
        state_dict: æ¨¡å‹çš„ state_dict
    
    Returns:
        dict: {'has_lora': bool, 'rank': int or None, 'lora_keys': list}
    """
    lora_keys = [k for k in state_dict.keys() if 'lora' in k]
    
    if not lora_keys:
        return {'has_lora': False, 'rank': None, 'lora_keys': []}
    
    # å˜—è©¦å¾ lora_A çš„å½¢ç‹€æ¨æ–· rank
    rank = None
    for key in lora_keys:
        if 'lora_A' in key:
            rank = state_dict[key].shape[0]
            break
    
    return {
        'has_lora': True,
        'rank': rank,
        'lora_keys': lora_keys,
        'num_lora_layers': len([k for k in lora_keys if 'lora_A' in k])
    }


def get_lora_state_dict(model):
    """
    åªæå– LoRA åƒæ•¸ï¼ˆä¸åŒ…å«åŸå§‹æ¬Šé‡ï¼‰
    
    Args:
        model: åŒ…å« LoRA çš„æ¨¡å‹
    
    Returns:
        dict: åªåŒ…å« LoRA åƒæ•¸çš„ state_dict
    """
    lora_state_dict = {}
    for name, param in model.named_parameters():
        if 'lora' in name and param.requires_grad:
            lora_state_dict[name] = param.data
    return lora_state_dict


def load_lora_weights(model, lora_state_dict, strict=True):
    """
    åªè¼‰å…¥ LoRA æ¬Šé‡ï¼ˆä¸è¼‰å…¥åŸå§‹æ¬Šé‡ï¼‰
    
    Args:
        model: å·²ç¶“æ³¨å…¥ LoRA çš„æ¨¡å‹
        lora_state_dict: LoRA æ¬Šé‡
        strict: æ˜¯å¦åš´æ ¼æª¢æŸ¥
    """
    # éæ¿¾å‡º LoRA åƒæ•¸
    model_lora_keys = [name for name, _ in model.named_parameters() if 'lora' in name]
    
    missing_keys = []
    unexpected_keys = []
    
    for key in lora_state_dict.keys():
        if key not in model_lora_keys:
            unexpected_keys.append(key)
    
    for key in model_lora_keys:
        if key not in lora_state_dict:
            missing_keys.append(key)
        else:
            # è¼‰å…¥åƒæ•¸
            param = dict(model.named_parameters())[key]
            param.data.copy_(lora_state_dict[key])
    
    if strict and (missing_keys or unexpected_keys):
        error_msg = f"Error loading LoRA weights:\n"
        if missing_keys:
            error_msg += f"  Missing keys: {missing_keys}\n"
        if unexpected_keys:
            error_msg += f"  Unexpected keys: {unexpected_keys}\n"
        raise RuntimeError(error_msg)
    
    return model


if __name__ == "__main__":
    # ç°¡å–®æ¸¬è©¦
    print("Testing LoRA implementation...\n")
    
    # å‰µå»ºä¸€å€‹ç°¡å–®çš„æ¸¬è©¦æ¨¡å‹
    class SimpleModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.qkv = nn.Linear(512, 1536)
            self.proj = nn.Linear(512, 512)
            self.fc = nn.Linear(512, 256)
        
        def forward(self, x):
            return self.fc(self.proj(self.qkv(x)))
    
    model = SimpleModel()
    print("Original model:")
    print(model)
    
    # æ³¨å…¥ LoRA
    model = inject_lora(model, rank=8, alpha=8.0)
    
    # æ‰“å°åƒæ•¸çµ±è¨ˆ
    print_lora_parameters(model)
    
    # æ¸¬è©¦å‰å‘å‚³æ’­
    x = torch.randn(4, 512)
    y = model(x)
    print(f"Output shape: {y.shape}")
    print("\nâœ… All tests passed!")
